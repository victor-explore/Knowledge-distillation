# Knowledge-distillation
This repository implements a pipeline to train a ResNet-50 on an animal image dataset, evaluate its performance, and distill its knowledge to a smaller MLP model using knowledge distillation techniques
